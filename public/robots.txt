Here is a simple robots.txt file with two rules, explained below:

# Group 1
User-agent: Googlebot
Disallow: /nogooglebot/

# Group 2
User-agent: *
Allow: /

Sitemap: http://www.example.com/sitemap.xml
 

Explanation:

The user agent named "Googlebot" crawler should not crawl the folder http://example.com/nogooglebot/ or any subdirectories.
All other user agents can access the entire site. (This could have been omitted and the result would be the same, as full access is the assumption.)
The site's Sitemap file is located at http://www.example.com/sitemap.xml